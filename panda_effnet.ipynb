{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import PIL.Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from apex import amp\n",
    "import torchvision\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, RandomSampler, SequentialSampler\n",
    "from efficientnet_pytorch import model as enet\n",
    "import albumentations\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from tqdm.notebook import tqdm\n",
    "DEVICE = torch.device('cuda')\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(i, torch.cuda.get_device_name(i))\n",
    "    print('  allocated:', round(torch.cuda.memory_allocated(i) / 1024 ** 3, 1), 'GB')\n",
    "    print('  cached:   ', round(torch.cuda.memory_cached(i) / 1024 ** 3, 1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = '.'\n",
    "DATA_PATH = '{}/data'.format(MAIN_PATH)\n",
    "LABELS_PATH = '{}/train.csv'.format(DATA_PATH)\n",
    "DEBUG = False\n",
    "APEX = True\n",
    "M_PARAMS = {\n",
    "    'MODEL_VER': 'v28',\n",
    "    'TILES_DIR': '{}/train_images_tiles_q1_256_36/'.format(DATA_PATH),\n",
    "    'ENET_TYPE': 'efficientnet-b1',\n",
    "    'N_FOLDS': 5,\n",
    "    'TILE_SIZE': 256,\n",
    "    'N_TILES': 25,\n",
    "    'BATCH_SIZE': 6,\n",
    "    'N_WORKERS': 12,\n",
    "    'OUT_DIM': 5,\n",
    "    'LR': 3e-4,\n",
    "    'N_EPOCHS': 10 if DEBUG else 30,\n",
    "    'WARMUP': True,\n",
    "    'SEED': 2020,\n",
    "    'DROPOUT': .4,\n",
    "    'RAND': False,\n",
    "    'COMMENTS': 'only affine effnet_ no bn double drop'\n",
    "}\n",
    "print(M_PARAMS)\n",
    "MODELS_PATH = '{}/effnet_models_{}'.format(MAIN_PATH, M_PARAMS['MODEL_VER'])\n",
    "if not os.path.exists(MODELS_PATH):\n",
    "    os.mkdir(MODELS_PATH)\n",
    "    print('created:', MODELS_PATH)\n",
    "PRETRAINED_MODEL = {\n",
    "    'efficientnet-b0': './bbs/efficientnet-b0-355c32eb.pth',\n",
    "    'efficientnet-b1': './bbs/efficientnet-b1-f1951068.pth',\n",
    "    'efficientnet-b2': './bbs/efficientnet-b2-8bb594d6.pth',\n",
    "    'efficientnet-b3': './bbs/efficientnet-b3-5fb5a3c3.pth'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    df_train = pd.read_csv(os.path.join(DATA_PATH, 'train_dsph.csv')).sample(100).reset_index(drop=True)\n",
    "else:\n",
    "    df_train = pd.read_csv(os.path.join(DATA_PATH, 'train_dsph.csv'))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(M_PARAMS['N_FOLDS'], shuffle=True, random_state=M_PARAMS['SEED'])\n",
    "df_train['fold'] = -1\n",
    "for i, (train_idx, valid_idx) in enumerate(skf.split(df_train, df_train['isup_grade'])):\n",
    "    df_train.loc[valid_idx, 'fold'] = i\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EffNet(nn.Module):\n",
    "    def __init__(self, backbone, out_dim):\n",
    "        super(EffNet, self).__init__()\n",
    "        self.enet = enet.EfficientNet.from_name(backbone)\n",
    "        self.enet.load_state_dict(torch.load(PRETRAINED_MODEL[backbone]))\n",
    "        #self.enet = enet.EfficientNet.from_pretrained(backbone)\n",
    "        nc = self.enet._fc.in_features\n",
    "        self.myfc = nn.Linear(nc, out_dim)\n",
    "        self.enet._fc = nn.Identity()\n",
    "        self.enet = nn.DataParallel(self.enet)\n",
    "    def forward(self, x):\n",
    "        x = self.enet(x)\n",
    "        x = self.myfc(x)\n",
    "        return x\n",
    "class EffNet_(nn.Module):\n",
    "    def __init__(self, backbone, out_dim):\n",
    "        super(EffNet_, self).__init__()\n",
    "        self.enet = enet.EfficientNet.from_name(backbone)\n",
    "        self.enet.load_state_dict(torch.load(PRETRAINED_MODEL[backbone]))\n",
    "        nc = self.enet._fc.in_features\n",
    "        self.enet._fc = nn.Identity()\n",
    "        self.enet = nn.DataParallel(self.enet)\n",
    "        self.myfc = nn.DataParallel(\n",
    "            nn.Sequential(\n",
    "                nn.Dropout(M_PARAMS['DROPOUT']),\n",
    "                nn.Linear(nc, int(nc / 4)),\n",
    "                nn.ELU(),\n",
    "                #nn.BatchNorm1d(int(nc / 4)), \n",
    "                nn.Dropout(M_PARAMS['DROPOUT']),\n",
    "                nn.Linear(int(nc / 4), out_dim)\n",
    "            )\n",
    "        )\n",
    "    def extract(self, x):\n",
    "        return self.enet(x)\n",
    "    def forward(self, x):\n",
    "        x = self.extract(x)\n",
    "        x = self.myfc(x)\n",
    "        return x\n",
    "class ResNext(nn.Module):\n",
    "    def __init__(self, out_dim):\n",
    "        super(ResNext, self).__init__()\n",
    "        self.rsnxt = torchvision.models.resnext50_32x4d(pretrained=True)\n",
    "        nc = self.rsnxt.fc.in_features\n",
    "        self.rsnxt.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(nc, int(nc / 4)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(int(nc / 4)), \n",
    "            nn.Dropout(.4),\n",
    "            nn.Linear(int(nc / 4), out_dim)\n",
    "        )\n",
    "        self.rsnxt = nn.DataParallel(self.rsnxt)\n",
    "    def forward(self, x):\n",
    "        x = self.rsnxt(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tiles(img_id, n_tiles):\n",
    "        result = []\n",
    "        seq_imgs = []\n",
    "        for i in range(n_tiles):\n",
    "            img_path = '{}/{}_{}.png'.format(M_PARAMS['TILES_DIR'], img_id, i)\n",
    "            img = cv2.imread(img_path)\n",
    "            if not np.any(img):\n",
    "                print('no img file read:', img_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            seq_imgs.append(img)\n",
    "        for i in range(len(seq_imgs)):\n",
    "            result.append({'img':seq_imgs[i], 'idx':i})\n",
    "        return result\n",
    "class PANDADataset(Dataset):\n",
    "    def __init__(self, df, tile_size, n_tiles, tile_mode=0, rand=False,\n",
    "                 transform=None, transform_tile=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tile_size = tile_size\n",
    "        self.n_tiles = n_tiles\n",
    "        self.tile_mode = tile_mode\n",
    "        self.rand = rand\n",
    "        self.transform = transform\n",
    "        self.transform_tile = transform_tile\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        img_id = row.image_id\n",
    "        tiles = get_tiles(img_id, self.n_tiles)\n",
    "        if self.rand:\n",
    "            idxes = np.random.choice(list(range(self.n_tiles)), self.n_tiles, replace=False)\n",
    "        else:\n",
    "            idxes = list(range(self.n_tiles))\n",
    "        n_row_tiles = int(np.sqrt(self.n_tiles))\n",
    "        images = np.zeros((self.tile_size * n_row_tiles, self.tile_size * n_row_tiles, 3))\n",
    "        for h in range(n_row_tiles):\n",
    "            for w in range(n_row_tiles):\n",
    "                i = h * n_row_tiles + w\n",
    "                if len(tiles) > idxes[i]:\n",
    "                    this_img = tiles[idxes[i]]['img']\n",
    "                else:\n",
    "                    this_img = np.ones((self.tile_size, self.tile_size, 3)).astype(np.uint8) * 255\n",
    "                this_img = (255 - this_img) / 255\n",
    "                if self.transform_tile is not None:\n",
    "                    this_img = self.transform_tile(image=this_img)['image']\n",
    "                h1 = h * self.tile_size\n",
    "                w1 = w * self.tile_size\n",
    "                images[h1 : h1 + self.tile_size, w1:w1 + self.tile_size] = this_img\n",
    "        images = images.astype(np.float32)\n",
    "        if self.transform is not None:\n",
    "            images = self.transform(image=images)['image']\n",
    "        images = images.transpose(2, 0, 1)\n",
    "        label = np.zeros(5).astype(np.float32)\n",
    "        label[:row.isup_grade] = 1.\n",
    "        return torch.tensor(images), torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = albumentations.Compose(\n",
    "    [\n",
    "        #albumentations.OneOf(\n",
    "        #    [\n",
    "        #        albumentations.RandomBrightness(limit=.1), \n",
    "        #        albumentations.RandomContrast(limit=.1), \n",
    "        #        albumentations.RandomGamma()\n",
    "        #    ], \n",
    "        #    p=.33\n",
    "        #),\n",
    "        albumentations.Transpose(p=.5),\n",
    "        albumentations.VerticalFlip(p=.5),\n",
    "        albumentations.HorizontalFlip(p=.5),\n",
    "        albumentations.Rotate(limit=15, p=.33)\n",
    "    ]\n",
    ")\n",
    "transforms_train_tile = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Transpose(p=.5),\n",
    "        albumentations.VerticalFlip(p=.5),\n",
    "        albumentations.HorizontalFlip(p=.5)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_show = PANDADataset(\n",
    "    df_train,\n",
    "    M_PARAMS['TILE_SIZE'], \n",
    "    M_PARAMS['N_TILES'], \n",
    "    tile_mode=0, \n",
    "    transform=transforms_train,\n",
    "    transform_tile=transforms_train_tile,\n",
    "    rand=M_PARAMS['RAND']\n",
    ")\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = (16, 4)\n",
    "f, axarr = plt.subplots(1, M_PARAMS['BATCH_SIZE'])\n",
    "for i in range(M_PARAMS['BATCH_SIZE']):\n",
    "    img, label = dataset_show.__getitem__(i)\n",
    "    axarr[i].imshow(1. - img.transpose(0, 1).transpose(1,2).squeeze())\n",
    "    axarr[i].set_title(str(sum(label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "def train_epoch(loader, optimizer):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    bar = tqdm(loader)\n",
    "    for (data, target) in bar:\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        loss_func = criterion\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(data)\n",
    "        loss = loss_func(logits, target)\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "        #loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_np = loss.detach().cpu().numpy()\n",
    "        train_loss.append(loss_np)\n",
    "        smooth_loss = sum(train_loss[-100:]) / min(len(train_loss), 100)\n",
    "        bar.set_description('loss: %.5f, smth: %.5f' % (loss_np, smooth_loss))\n",
    "    return train_loss\n",
    "def val_epoch(loader, get_output=False):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    LOGITS = []\n",
    "    PREDS = []\n",
    "    TARGETS = []\n",
    "    with torch.no_grad():\n",
    "        for (data, target) in tqdm(loader):\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            logits = model(data)\n",
    "            loss = criterion(logits, target)\n",
    "            pred = logits.sigmoid().sum(1).detach().round()\n",
    "            LOGITS.append(logits)\n",
    "            PREDS.append(pred)\n",
    "            TARGETS.append(target.sum(1))\n",
    "            val_loss.append(loss.detach().cpu().numpy())\n",
    "        val_loss = np.mean(val_loss)\n",
    "    LOGITS = torch.cat(LOGITS).cpu().numpy()\n",
    "    PREDS = torch.cat(PREDS).cpu().numpy()\n",
    "    TARGETS = torch.cat(TARGETS).cpu().numpy()\n",
    "    acc = (PREDS == TARGETS).mean() * 100.\n",
    "    qwk = cohen_kappa_score(PREDS, TARGETS, weights='quadratic')\n",
    "    qwk_k = cohen_kappa_score(\n",
    "        PREDS[df_valid['data_provider'] == 'karolinska'], \n",
    "        df_valid[df_valid['data_provider'] == 'karolinska'].isup_grade.values, \n",
    "        weights='quadratic'\n",
    "    )\n",
    "    qwk_r = cohen_kappa_score(\n",
    "        PREDS[df_valid['data_provider'] == 'radboud'], \n",
    "        df_valid[df_valid['data_provider'] == 'radboud'].isup_grade.values, \n",
    "        weights='quadratic'\n",
    "    )\n",
    "    print('QWK', qwk, 'QWK_k', qwk_k, 'QWK_r', qwk_r)\n",
    "    if get_output:\n",
    "        return LOGITS\n",
    "    else:\n",
    "        return val_loss, acc, qwk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred, target = [], []\n",
    "if DEBUG:\n",
    "    n_folds_train = 2\n",
    "else:\n",
    "    n_folds_train = M_PARAMS['N_FOLDS']\n",
    "start_folds_train = 0\n",
    "for fold_num in range(start_folds_train, n_folds_train):\n",
    "    print('-' * 20, 'fold:', fold_num, '-' * 20)\n",
    "    train_idx = np.where((df_train['fold'] != fold_num))[0]\n",
    "    valid_idx = np.where((df_train['fold'] == fold_num))[0]\n",
    "    df_this  = df_train.loc[train_idx]\n",
    "    df_valid = df_train.loc[valid_idx]\n",
    "    dataset_train = PANDADataset(\n",
    "        df_this, \n",
    "        M_PARAMS['TILE_SIZE'], \n",
    "        M_PARAMS['N_TILES'], \n",
    "        transform=transforms_train,\n",
    "        transform_tile=transforms_train_tile,\n",
    "        rand=M_PARAMS['RAND']\n",
    "    )\n",
    "    dataset_valid = PANDADataset(\n",
    "        df_valid, \n",
    "        M_PARAMS['TILE_SIZE'], \n",
    "        M_PARAMS['N_TILES']\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset_train, \n",
    "        batch_size=M_PARAMS['BATCH_SIZE'], \n",
    "        sampler=RandomSampler(dataset_train), \n",
    "        num_workers=M_PARAMS['N_WORKERS']\n",
    "    )\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        dataset_valid, \n",
    "        batch_size=M_PARAMS['BATCH_SIZE'], \n",
    "        sampler=SequentialSampler(dataset_valid), \n",
    "        num_workers=M_PARAMS['N_WORKERS']\n",
    "    )\n",
    "    model = EffNet_(M_PARAMS['ENET_TYPE'], out_dim=M_PARAMS['OUT_DIM']) \n",
    "    #model = ResNext(out_dim=M_PARAMS['OUT_DIM'])\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=M_PARAMS['LR'])\n",
    "    model, optimizer = amp.initialize(model, optimizer, opt_level='O1')\n",
    "    if M_PARAMS['WARMUP']:\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, \n",
    "            max_lr=M_PARAMS['LR'], \n",
    "            total_steps=M_PARAMS['N_EPOCHS'],\n",
    "            div_factor=(M_PARAMS['LR'] / 1e-5), \n",
    "            final_div_factor=1000,\n",
    "            pct_start=(2 / M_PARAMS['N_EPOCHS']),\n",
    "        )\n",
    "    else:\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, M_PARAMS['N_EPOCHS'])\n",
    "    print('train len:', len(dataset_train),'| val len:', len(dataset_valid))\n",
    "    qwk_max = 0\n",
    "    best_file = '{}/{}_best_fold{}.pth'.format(MODELS_PATH, M_PARAMS['ENET_TYPE'], fold_num)\n",
    "    for epoch in tqdm(range(M_PARAMS['N_EPOCHS']), desc='epochs'):\n",
    "        print(time.ctime(), 'epoch:', epoch)\n",
    "        train_loss = train_epoch(train_loader, optimizer)\n",
    "        val_loss, acc, qwk = val_epoch(valid_loader)\n",
    "        scheduler.step(epoch)\n",
    "        content = '{} epoch {}, lr: {:.8f}, train loss: {:.4f}, val loss: {:.4f}, acc: {:.1f}, QWK: {:.4f}'.format(\n",
    "                time.ctime(),\n",
    "                epoch, \n",
    "                optimizer.param_groups[0]['lr'], \n",
    "                np.mean(train_loss),\n",
    "                np.mean(val_loss),\n",
    "                acc,\n",
    "                qwk\n",
    "            )\n",
    "        print(content)\n",
    "        with open('{}/log_{}_fold{}.txt'.format(MODELS_PATH, M_PARAMS['ENET_TYPE'], fold_num), 'a') as appender:\n",
    "            appender.write(content + '\\n')\n",
    "        if qwk > qwk_max:\n",
    "            torch.save(model.state_dict(), best_file)\n",
    "            print('QWK improved {:.6f} --> {:.6f} model saved'.format(qwk_max, qwk))\n",
    "            qwk_max = qwk\n",
    "    with open('{}/log_{}_folds_all.txt'.format(MODELS_PATH, M_PARAMS['ENET_TYPE']), 'a') as appender:\n",
    "        appender.write('{} | fold: {} | max QWK: {:.6f}\\n'.format(M_PARAMS, fold_num, qwk_max))\n",
    "    torch.save(\n",
    "        model.state_dict(), \n",
    "        os.path.join('{}/{}_final_fold{}.pth'.format(MODELS_PATH, M_PARAMS['ENET_TYPE'], fold_num))\n",
    "    )\n",
    "    del model, dataset_train, dataset_valid, train_loader, valid_loader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Orange Python 3 (2 GPUs)",
   "language": "python",
   "name": "orange2gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
