{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import fastai\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import SaveModelCallback\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, './fastai_utils/')\n",
    "from radam import *\n",
    "from csvlogger import *\n",
    "from mish_activation import *\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import cohen_kappa_score,confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print(fastai.__version__)\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(i, torch.cuda.get_device_name(i))\n",
    "    print('  allocated:', round(torch.cuda.memory_allocated(i) / 1024 ** 3, 1), 'GB')\n",
    "    print('  cached:   ', round(torch.cuda.memory_cached(i) / 1024 ** 3, 1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 256\n",
    "bs = 8\n",
    "nfolds = 4\n",
    "SEED = 80\n",
    "N_WORKERS = 8\n",
    "N = 20\n",
    "MODEL_VER = 'v15'\n",
    "MAIN_PATH = '.'\n",
    "DATA_PATH = '{}/data'.format(MAIN_PATH)\n",
    "TRAIN = '{}/train_images_tiles_q1_256_36/'.format(DATA_PATH)\n",
    "LABELS = '{}/train.csv'.format(DATA_PATH)\n",
    "MODELS = '{}/fastai_models'.format(MAIN_PATH)\n",
    "if not os.path.exists('{}_{}'.format(MODELS, MODEL_VER)):\n",
    "    os.mkdir('{}_{}'.format(MODELS, MODEL_VER))\n",
    "    print('created:', '{}_{}'.format(MODELS, MODEL_VER))\n",
    "# v_ = sz 256 bs 6 N 12 folds 4 --> 0.8398 (1st fold)\n",
    "# v0 = sz 256 bs 8 N 8  folds 4 --> 0.8057 (1st fold)\n",
    "# v1 = sz 512 bs 6 N 3 folds 4 --> 0.7959 (1st fold)\n",
    "# v2 = sz 256 bs 5 N 14 folds 4 max_lr 1e-3 df 1000 --> .77998 (1st fold)\n",
    "# v3 = sz 256 bs 6 N 12 folds 4 max_lr 1e-3 df 1000 --> \n",
    "# v4 = sz 256_36 bs 6 N 12 folds 4 max_lr 3e-4 df 100 --> .8382(0), .8234(1), .8392(2), .8251(2), .8314(all) --> .84 LB\n",
    "# v5 = sz 256_36 bs 6 N 12 folds 4 max_lr 5e-4 df 500 ep 40 --> .8341 (1st fold)\n",
    "# v6 = sz 128_24_r2 bs 12 N 24 folds 4 max_lr 3e-4 df 100 ep 30 --> .8400 (1st fold)\n",
    "# v7 = sz 128_32 bs 8 N 32 folds 4 max_lr 3e-4 df 100 ep 30 --> .8422(0) .8380(1) .8423(2) .8340(3) .8391(all) --> .85 LB\n",
    "# v8 = sz 256_36 bs 12(x2) N 12 folds 4 max_lr 3e-4 df 100 --> .8400(0)\n",
    "# v9 = sz 256_36 bs 6(x2) N 22 folds 4 max_lr 3e-4 df 100 ep 30 --> .8264(0)\n",
    "# v10 = sz 256_36 bs 8(x2) N 18 folds 4 max_lr 3e-4 df 100 ep 30 --> .8510(0)\n",
    "# v11 = sz 156_64 bs 8(x2) N 48 folds 4 max_lr 3e-4 df 100 ep 30 --> .8527(0)\n",
    "# v12 = sz 156_64 bs 12(x2) N 36 folds 4 max_lr 3e-4 df 100 ep 30 --> .8423(0)\n",
    "# v13 = sz 156_64 bs 12(x2) N 54 folds 4 max_lr 3e-4 df 100 ep 40 --> .8493(0)\n",
    "# v14 =  sz 256_36 bs 8(x2) N 20 folds 4 max_lr 3e-4 df 100 ep 40--> .8626(0) .8513(1) .8539(2) .8547(3) .8556(all) \n",
    "# --> .86 LB\n",
    "# v15 = \n",
    "print('256x256x18 img size:', 256 * 256 * 18)\n",
    "print('128x128x32 img size:', 128 * 128 * 32)\n",
    "print('current img size:', sz * sz * N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(LABELS).set_index('image_id')\n",
    "files = sorted(set([p[:32] for p in os.listdir(TRAIN) if '.ipynb' not in p]))\n",
    "df = df.loc[files]\n",
    "df = df.reset_index()\n",
    "splits = StratifiedKFold(n_splits=nfolds, random_state=SEED, shuffle=True)\n",
    "splits = list(splits.split(df,df.isup_grade))\n",
    "folds_splits = np.zeros(len(df)).astype(np.int)\n",
    "for i in range(nfolds): folds_splits[splits[i][1]] = i\n",
    "df['split'] = folds_splits\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q1_256_36\n",
    "mean = torch.tensor([1 - .85506157, 1 - .7035249, 1 - .80203127])\n",
    "std = torch.tensor([.40011922, .52504386, .42675745])\n",
    "# q1_512_16\n",
    "#mean = torch.tensor([1 - .91066496, 1 - .82012713, 1 - .87912669])\n",
    "#std = torch.tensor([.37697129, .50629405, .41176592])\n",
    "# q1_256_24\n",
    "#mean = torch.tensor([1 - .82934477, 1 - .64875879, 1 - .76593138])\n",
    "#std = torch.tensor([.40447862, .51985678, .42330205])\n",
    "# q1_128_32\n",
    "#mean = torch.tensor([1 - .75793065, 1 - .5088926, 1 - .67191824])\n",
    "#std = torch.tensor([.41296871, .47099853, .39308129])\n",
    "# q1_128_24_r2\n",
    "#mean = torch.tensor([1 - .8296201, 1 - .64903966, 1 - .76621777])\n",
    "#std = torch.tensor([.39454631, .51460749, .41747009])\n",
    "# q1_156_64\n",
    "#mean = torch.tensor([1 - .81198481, 1 - .61055975, 1 - .74127657])\n",
    "#std = torch.tensor([.40365266, .50938881, .41515295])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_image(fn:PathOrStr, div:bool=True, convert_mode:str='RGB', cls:type=Image,\n",
    "        after_open:Callable=None)->Image:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", UserWarning) # EXIF warning from TiffPlugin\n",
    "        x = PIL.Image.open(fn).convert(convert_mode)\n",
    "    if after_open: x = after_open(x)\n",
    "    x = pil2tensor(x,np.float32)\n",
    "    if div: x.div_(255)\n",
    "    return cls(1.0-x) #invert image for zero padding\n",
    "class MImage(ItemBase):\n",
    "    def __init__(self, imgs):\n",
    "        self.obj, self.data = \\\n",
    "          (imgs), [(imgs[i].data - mean[...,None,None])/std[...,None,None] for i in range(len(imgs))]\n",
    "    def apply_tfms(self, tfms,*args, **kwargs):\n",
    "        for i in range(len(self.obj)):\n",
    "            self.obj[i] = self.obj[i].apply_tfms(tfms, *args, **kwargs)\n",
    "            self.data[i] = (self.obj[i].data - mean[...,None,None])/std[...,None,None]\n",
    "        return self\n",
    "    def __repr__(self): return f'{self.__class__.__name__} {img.shape for img in self.obj}'\n",
    "    def to_one(self):\n",
    "        img = torch.stack(self.data,1)\n",
    "        img = img.view(3,-1,N,sz,sz).permute(0,1,3,2,4).contiguous().view(3,-1,sz*N)\n",
    "        return Image(1.0 - (mean[...,None,None]+img*std[...,None,None]))\n",
    "class MImageItemList(ImageList):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    def __len__(self)->int: return len(self.items) or 1 \n",
    "    def get(self, i):\n",
    "        fn = Path(self.items[i])\n",
    "        fnames = [Path(str(fn)+'_'+str(i)+'.png')for i in range(N)]\n",
    "        imgs = [open_image(fname, convert_mode=self.convert_mode, after_open=self.after_open)\n",
    "               for fname in fnames]\n",
    "        return MImage(imgs)\n",
    "    def reconstruct(self, t):\n",
    "        return MImage([mean[...,None,None]+_t*std[...,None,None] for _t in t])\n",
    "    def show_xys(self, xs, ys, figsize:Tuple[int,int]=(300,50), **kwargs):\n",
    "        rows = min(len(xs),8)\n",
    "        fig, axs = plt.subplots(rows,1,figsize=figsize)\n",
    "        for i, ax in enumerate(axs.flatten() if rows > 1 else [axs]):\n",
    "            xs[i].to_one().show(ax=ax, y=ys[i], **kwargs)\n",
    "        plt.tight_layout()\n",
    "def MImage_collate(batch:ItemsList)->Tensor:\n",
    "    result = torch.utils.data.dataloader.default_collate(to_data(batch))\n",
    "    if isinstance(result[0],list):\n",
    "        result = [torch.stack(result[0],1),result[1]]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(fold=0):\n",
    "    return (\n",
    "        MImageItemList.from_df(df, path='.', folder=TRAIN, cols='image_id')\n",
    "        .split_by_idx(df.index[df.split == fold].tolist())\n",
    "        .label_from_df(cols=['isup_grade'])\n",
    "        .transform(\n",
    "            get_transforms(\n",
    "                flip_vert=True,\n",
    "                max_rotate=15,\n",
    "                max_warp=None\n",
    "            ),\n",
    "            size=sz,\n",
    "            padding_mode='zeros'\n",
    "        ).databunch(\n",
    "            bs=bs, \n",
    "            num_workers=N_WORKERS\n",
    "        )\n",
    "    )\n",
    "data = get_data(0)\n",
    "#data.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.one_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, arch='resnext50_32x4d_ssl', n=6, pre=True):\n",
    "        super().__init__()\n",
    "        m = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', arch)\n",
    "        self.enc = nn.DataParallel(nn.Sequential(*list(m.children())[:-2]))\n",
    "        nc = list(m.children())[-1].in_features\n",
    "        self.head = nn.DataParallel(nn.Sequential(\n",
    "            AdaptiveConcatPool2d(),\n",
    "            Flatten(),\n",
    "            nn.Linear(2 * nc, 512),\n",
    "            Mish(),\n",
    "            nn.BatchNorm1d(512), \n",
    "            nn.Dropout(.4),\n",
    "            nn.Linear(512, n)\n",
    "        ))\n",
    "    def forward(self, *x):\n",
    "        shape = x[0].shape\n",
    "        n = len(x)\n",
    "        x = torch.stack(x,1).view(\n",
    "            -1,\n",
    "            shape[1],\n",
    "            shape[2],\n",
    "            shape[3]\n",
    "        )\n",
    "        #x: bs*N x 3 x 128 x 128\n",
    "        x = self.enc(x)\n",
    "        #x: bs*N x C x 4 x 4\n",
    "        shape = x.shape\n",
    "        #concatenate the output for tiles into a single map\n",
    "        x = x.view(\n",
    "            -1,\n",
    "            n,\n",
    "            shape[1],\n",
    "            shape[2],\n",
    "            shape[3]\n",
    "        ).permute(0, 2, 1, 3, 4).contiguous().view(\n",
    "            -1,\n",
    "            shape[1],\n",
    "            shape[2] * n,\n",
    "            shape[3]\n",
    "        )\n",
    "        #x: bs x C x N*4 x 4\n",
    "        x = self.head(x)\n",
    "        #x: bs x n\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'RNXT50'\n",
    "pred, target = [], []\n",
    "NFOLDS = nfolds\n",
    "for fold in range(NFOLDS):\n",
    "    print('-' * 20, 'fold:', fold, '-' * 20)\n",
    "    data = get_data(fold)\n",
    "    model = Model()\n",
    "    learn = Learner(\n",
    "        data, \n",
    "        model, \n",
    "        loss_func=nn.CrossEntropyLoss(), \n",
    "        opt_func=Over9000, \n",
    "        metrics=[KappaScore(weights='quadratic')]\n",
    "    ).to_fp16()\n",
    "    logger = CSVLogger(learn, '{}_{}/log_{}_{}'.format(MODELS, MODEL_VER, fname, fold))\n",
    "    learn.clip_grad = 1.0\n",
    "    learn.split([model.head])\n",
    "    learn.unfreeze()\n",
    "    learn.fit_one_cycle(\n",
    "        40, \n",
    "        max_lr=3e-4,\n",
    "        div_factor=100, \n",
    "        pct_start=0.0, \n",
    "        callbacks = [\n",
    "            SaveModelCallback(\n",
    "                learn, \n",
    "                name='model_{}'.format(MODEL_VER), \n",
    "                monitor='kappa_score'\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    torch.save(learn.model.state_dict(), '{}_{}/{}_fold{}.pth'.format(MODELS, MODEL_VER, fname, fold))\n",
    "    learn.model.eval()\n",
    "    pred_fold, target_fold = [], []\n",
    "    with torch.no_grad():\n",
    "        for step, (x, y) in progress_bar(\n",
    "            enumerate(data.dl(DatasetType.Valid)),\n",
    "            total=len(data.dl(DatasetType.Valid))\n",
    "        ):\n",
    "            p = learn.model(*x)\n",
    "            pred_step = p.float().cpu()\n",
    "            target_step = y.cpu()\n",
    "            pred.append(pred_step)\n",
    "            target.append(target_step)\n",
    "            pred_fold.append(pred_step)\n",
    "            target_fold.append(target_step)\n",
    "    p = torch.argmax(torch.cat(pred_fold,0),1)\n",
    "    t = torch.cat(target_fold)\n",
    "    print(cohen_kappa_score(t,p,weights='quadratic'))\n",
    "    print(confusion_matrix(t,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.argmax(torch.cat(pred,0),1)\n",
    "t = torch.cat(target)\n",
    "print(cohen_kappa_score(t,p,weights='quadratic'))\n",
    "print(confusion_matrix(t,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Orange Python 3 (2 GPUs)",
   "language": "python",
   "name": "orange2gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
